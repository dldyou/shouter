{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\speed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\whisper\\timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Hello everyone, my name is Kim Hyun-won. We have to do some project. Yeah, this is a sample MP3 file for above project.\n"
     ]
    }
   ],
   "source": [
    "import whisper as ws\n",
    "\n",
    "model = ws.load_model(\"base\")\n",
    "\n",
    "#이걸 구해야 할건디,,,,\n",
    "audio = ws.load_audio(\"test2.m4a\")\n",
    "audio = ws.pad_or_trim(audio)\n",
    "\n",
    "mel = ws.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f'Detected language: {max(probs, key=probs.get)}')\n",
    "\n",
    "options = ws.DecodingOptions()\n",
    "result = ws.decode(model, mel, options)\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA device detection test\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "\n",
    "for i in range(device_count):\n",
    "    device = torch.device(f\"cuda:{i}\")\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(device)}\")\n",
    "    print(torch.cuda.get_device_properties(device).total_memory / 1024**3, ' GB')\n",
    "    print(torch.cuda.get_device_properties(device).total_memory / 1024**3, ' GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\speed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\speed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량 (bytes): 5967801856\n"
     ]
    }
   ],
   "source": [
    "# VRAM stress test\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# load ResNet-152 model\n",
    "model = models.resnet152(pretrained=True)\n",
    "\n",
    "# move model into GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "\n",
    "# set large batch size\n",
    "batch_size = 32\n",
    "\n",
    "# generate input data\n",
    "input_data = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "# run\n",
    "output = model(input_data)\n",
    "\n",
    "# output\n",
    "print(\"GPU 메모리 사용량 (bytes):\", torch.cuda.max_memory_allocated(device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\whisper\\timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\n",
      "Processor count: 12\n",
      "Memory: 15 GB\n",
      "CUDA Device 0: NVIDIA GeForce MX550\n",
      "MoviePy - Writing audio in C:\\Code\\shouter\\shouter\\shouter\\test\\audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "# performance test - GPU mode\n",
    "import whisper as ws\n",
    "import torch, time, platform, psutil, extract\n",
    "\n",
    "model = ws.load_model(\"medium\")\n",
    "\n",
    "def get_device_info():\n",
    "    print(f'CPU: {platform.processor()}')\n",
    "    print(f'Processor count: {psutil.cpu_count(logical=False)}')\n",
    "    print(f'Memory: {psutil.virtual_memory().total // (1024**3)} GB')\n",
    "    \n",
    "    device_count = torch.cuda.device_count()\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        print(f'CUDA Device {i}: {torch.cuda.get_device_name(device)}')\n",
    "\n",
    "def start(path):\n",
    "    # 오디오 불러오기 및 trim 진행\n",
    "    audio = ws.load_audio(path)\n",
    "    audio = ws.pad_or_trim(audio)\n",
    "\n",
    "    mel = ws.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "    _, probs = model.detect_language(mel)\n",
    "    print(f'Detected language: {max(probs, key=probs.get)}')\n",
    "\n",
    "    options = ws.DecodingOptions(fp16 = False)\n",
    "    # result = ws.decode(model, mel, options)\n",
    "    result = model.transcribe(audio)\n",
    "\n",
    "    #print(type(result))\n",
    "    # print(result['segments'])\n",
    "    \n",
    "    ret = []\n",
    "    for data in result['segments']:\n",
    "        start_time = round(data['start'], 2)\n",
    "        end_time = round(data['end'], 2)\n",
    "        text = data['text']\n",
    "        ret.append([start_time, end_time, text])\n",
    "        \n",
    "        print(f'{start_time} ~ {end_time}: {text}')\n",
    "        \n",
    "    return ret\n",
    "\n",
    "#run task\n",
    "get_device_info()\n",
    "extract.extract()\n",
    "start_t = time.time()\n",
    "start('audio.mp3')\n",
    "end_t = time.time()\n",
    "print(f'{end_t - start_t:.5f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwhisper\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mws\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mextract\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m ws\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39mmedium\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_info\u001b[39m():\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCPU: \u001b[39m\u001b[39m{\u001b[39;00mplatform\u001b[39m.\u001b[39mprocessor()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\whisper\\__init__.py:144\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m not found; available models = \u001b[39m\u001b[39m{\u001b[39;00mavailable_models()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     io\u001b[39m.\u001b[39mBytesIO(checkpoint_file) \u001b[39mif\u001b[39;00m in_memory \u001b[39melse\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m ) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m--> 144\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(fp, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    145\u001b[0m \u001b[39mdel\u001b[39;00m checkpoint_file\n\u001b[0;32m    147\u001b[0m dims \u001b[39m=\u001b[39m ModelDimensions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheckpoint[\u001b[39m\"\u001b[39m\u001b[39mdims\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:1083\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[1;32m-> 1083\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, map_location)\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mUntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[1;32mc:\\Users\\speed\\anaconda3\\envs\\shouter\\lib\\site-packages\\torch\\_utils.py:84\u001b[0m, in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mUntypedStorage(\n\u001b[0;32m     82\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     untyped_storage\u001b[39m.\u001b[39;49mcopy_(\u001b[39mself\u001b[39;49m, non_blocking)\n\u001b[0;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# performance test - CPU mode\n",
    "import platform, psutil, time\n",
    "import whisper as ws\n",
    "import extract\n",
    "\n",
    "model = ws.load_model(\"medium\")\n",
    "\n",
    "def get_device_info():\n",
    "    print(f'CPU: {platform.processor()}')\n",
    "    print(f'Processor count: {psutil.cpu_count(logical=False)}')\n",
    "    print(f'Memory: {psutil.virtual_memory().total // (1024**3)} GB')\n",
    "\n",
    "def start(path):\n",
    "    #모델 불러오기\n",
    "    global model\n",
    "    \n",
    "    # 오디오 불러오기 및 trim 진행\n",
    "    audio = ws.load_audio(path)\n",
    "    audio = ws.pad_or_trim(audio)\n",
    "\n",
    "    mel = ws.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "    _, probs = model.detect_language(mel)\n",
    "    print(f'Detected language: {max(probs, key=probs.get)}')\n",
    "\n",
    "    options = ws.DecodingOptions(fp16 = False)\n",
    "    # result = ws.decode(model, mel, options)\n",
    "    result = model.transcribe(audio)\n",
    "\n",
    "    #print(type(result))\n",
    "    # print(result['segments'])\n",
    "    \n",
    "    ret = []\n",
    "    for data in result['segments']:\n",
    "        start_time = round(data['start'], 2)\n",
    "        end_time = round(data['end'], 2)\n",
    "        text = data['text']\n",
    "        ret.append([start_time, end_time, text])\n",
    "        \n",
    "        print(f'{start_time} ~ {end_time}: {text}')\n",
    "        \n",
    "    return ret\n",
    "\n",
    "#run task\n",
    "get_device_info()\n",
    "extract.extract()\n",
    "start_t = time.time()\n",
    "start('audio.mp3')\n",
    "end_t = time.time()\n",
    "print(f'{end_t - start_t:.5f} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
